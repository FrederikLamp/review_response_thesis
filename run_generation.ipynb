{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frederiklamp/OneDrive - Syddansk Universitet/Kandidat_Data_Science_SDU/Specialemappe/Nic_Chr_group/project/.env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-05-31 12:15:52.259473: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n",
    "import torch\n",
    "\n",
    "# Specify the directory where your fine-tuned model and tokenizer are saved\n",
    "model_dir = \"./gen_model/checkpoint-230\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_dir,)\n",
    "\n",
    "# Load the model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_dir)\n",
    "\n",
    "# Optionally, you can move the model to the appropriate device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the test data\n",
    "test_data = pickle.load(open(\"./data/test_data_10p.pkl\", \"rb\")) # load from pickle file to avoid reprocessing\n",
    "\n",
    "print(test_data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to reprocess the test data strings into 3 sections: user, review, reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_sections(text):\n",
    "    # Define the tags\n",
    "    tags = [\"### Bruger:\", \"### Anmeldelse:\", \"### Svar:\"]\n",
    "    \n",
    "    # Initialize an empty list to store the sections\n",
    "    sections = [\"\", \"\", \"\"]\n",
    "    \n",
    "    # Split the text by the first tag to get the user's name section\n",
    "    parts = text.split(tags[0])\n",
    "    if len(parts) > 1:\n",
    "        sections[0] = parts[1].split(tags[1])[0].strip()\n",
    "    \n",
    "    # Split the text by the second tag to get the review section\n",
    "    parts = text.split(tags[1])\n",
    "    if len(parts) > 1:\n",
    "        sections[1] = parts[1].split(tags[2])[0].strip()\n",
    "    \n",
    "    # Split the text by the third tag to get the response section\n",
    "    parts = text.split(tags[2])\n",
    "    if len(parts) > 1:\n",
    "        sections[2] = parts[1].strip()\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_reviews = [separate_sections(review) for review in test_data[:10]]\n",
    "for i in example_reviews:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for stripping excess generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###For testing\n",
    "def prompt_template(user, review):\n",
    "    return f\"### Bruger:\\n{user}\\n\\n### Anmeldelse:\\n{review}\\n\\n### Svar:\\nKære {user}\\n\"\n",
    "\n",
    "import re\n",
    "\n",
    "# Generate text but only return one reply and strip the rest\n",
    "def extract_first_lines(generated_text):\n",
    "    # Defining regex patterns for each of the three tags\n",
    "    patterns = {\n",
    "        '### Bruger:': r'### Bruger:\\s*(.*)',\n",
    "        '### Anmeldelse:': r'### Anmeldelse:\\s*(.*)'\n",
    "    }\n",
    "    \n",
    "    # Extract the first occurrence of '### Bruger:' and '### Anmeldelse:'\n",
    "    extracted_content = {}\n",
    "    for tag, pattern in patterns.items():\n",
    "        match = re.search(pattern, generated_text, re.MULTILINE)\n",
    "        if match:\n",
    "            extracted_content[tag] = match.group(1).strip()\n",
    "    \n",
    "    # Extract the entire text between '### Svar:' and the next '### Bruger:' or '### Anmeldelse:'\n",
    "    response_pattern = r'### Svar:\\s*(.*?)((?=### Bruger:|### Anmeldelse:|### svar)|$)'\n",
    "    response_match = re.search(response_pattern, generated_text, re.DOTALL)\n",
    "    if response_match:\n",
    "        extracted_content['### Svar:'] = response_match.group(1).strip()\n",
    "    \n",
    "    return extracted_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get the predictions (generated replies) from the model on the first part of the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for review in example_reviews:\n",
    "    # Generate text\n",
    "    user = review[0]\n",
    "    review_sequence = review[1]\n",
    "\n",
    "    # Using pipeline\n",
    "    generator = pipeline(\"text-generation\", model= model, tokenizer=tokenizer)\n",
    "\n",
    "    prompt = prompt_template(user, review_sequence)\n",
    "\n",
    "    # get the output\n",
    "    output = generator(prompt, max_length= 500, do_sample=True, temperature=0.4) # low temp for less randomness\n",
    "    \n",
    "    # select output text\n",
    "    generated_text = (output[0]['generated_text'])\n",
    "\n",
    "    # make sure to remove excess text (only one of each '###' tag per review)\n",
    "    extracted_content = extract_first_lines(generated_text)\n",
    "\n",
    "    # get the text from the response tag: '###svar: ' (actual reply)\n",
    "    response = extracted_content['### Svar:']\n",
    "\n",
    "    #append the response to the predictions list\n",
    "    predictions.append('### Svar :' + response)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in predictions[:5]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to experiment the performance of the model by generating outputs to different types of reviews.\n",
    "\n",
    "The review should be related to a customer experience with buying a car or from  a mechanic service appointment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Test sequence\n",
    "one_star_reviews = {\n",
    "    'bruger': ['Birthe', 'Michael', 'Mette'], \n",
    "    'review': [\"Holder ikke hvad de lover. Bryder aftaler og giver kun en problemer og gæld med hjem. Kan ikke stole på dem.\",\n",
    "    \"Jeg sender min bil til service og syn hvor jeg så for et opkald om at mine stabilisator arme skal skiftes grundet slør samt skover og bremser bagpå skal skiftes, det forlanger de omkring 7000 for. Hvortil jeg afviser at de skal lave den. Også af den grund at jeg ikke har haft noget slør i bilen inden den kom til service. \\nDa jeg så får bilen tilbage er der pludselig lyde fra mine stabilisator arme samt at der pludselig er kommet slør i rettet.\",\n",
    "    \"Vi leverede en bil til service og skade reparation. Da vi hentede kunne bremsen trædes helt ned. Retur igen. Da vi hentede anden gang lyste en masse lamper, så retur igen. \\n\\nVi er utilfredse med at alt ikke bliver lavet i første omgang, når vi afleverer en bil.\"\n",
    "    ],\n",
    "}\n",
    "two_star_reviews = {\n",
    "    'bruger': ['Louise', 'Bertram', 'Simon'],\n",
    "    'review': [\"Har købt en suzuki baleno for 2 år siden han tog en toyota aygo i bytte kan regne ud at det var alt for lidt vi fik for den og baleno var alt for dyr i forhold til hvad den er værd idag selvom vi lagde en stor udbetaling.\\nVi føler os snydt, derfor bliver det ikke et sted vi handler igen\",\n",
    "               \"Venlig betjening men skuffende resultat. 6 år gammel bil bliver kasseret til syn mindre end 3 uger efter på en mekanisk fejl.\",\n",
    "               \"En bule i skærmen skal repareres, sammen med et service. efter 2 ugers ventetid og 4 forsøg på at ringe værkstedet op, får jeg at vide, at der ikke er nogen tidshorisont for min bil. De har fået den forkerte reservedel og aner ikke hvornår de får den rigtige.\"\n",
    "    ],\n",
    "}\n",
    "three_star_reviews = {\n",
    "    'bruger': ['Connie', 'Mads', 'Gitte'],\n",
    "    'review': [\"Ok service. God pris når man skal have fundet fejlen på bilen\",\n",
    "               \"Service og rådgivning fint. Desværre var efterfølgende prisoverslag markant dyrere end tilsvarende værksteder så de ekstra ting som skulle laves måtte jeg løse andetsteds.\",\n",
    "               \"Min reparation blev diagnosticeret og reservedele ( nogle slanger) hentet hjem, men af 2 omgange var det de forkerte, så jeg måtte vente 2 dage ekstra, herefter blev en ny fejl, der ville tredoble prisen fundet. \\nEfterfølgende blev slangerne fejlmonteret så diesel fossede ud i motor og under bilen. Eneste grund til de ikke får 1 stjerne for dette arbejde er deres meget fine måde at være på under hele forløbet, beklagende og forstående. God service, meget tvivlsomt arbejde.\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "four_star_reviews = {\n",
    "    'bruger': ['Mikkel', 'Jens', 'Anders'],\n",
    "    'review': [\"Forsikringsreparation. Den bestilte forsikringsreparation ganske ok, fin bilvask - MEN lånebilen lod en del tilbage at ønske\",\n",
    "               \"Hurtig, god og serviceminded kontakt og booking.\\nReparationen er veludført og uden efterfølgende problemer. \\nOplevelsen med lånebilen var dog ikke fantastisk, der var 34 km tilbage i tanken da jeg fik den og koblingen virkede kun på den sidste cm af pedalgangen.\",\n",
    "               \"Ok oplevelse. \\nGod information hele processen igennem! \\nDe fik min gamle bil til at køre igen, jeg er glad\"\n",
    "    ]\n",
    "}\n",
    "five_star_reviews = {\n",
    "    'bruger': ['Casper', 'Sofie', 'Lars'],\n",
    "    'review': [\"Tager sig tid til at give grundig information om reparationer med videoer forklaringer mv. Meget venligt og serviceminded personale. Hurtig service. Jeg føler mig generelt meget tryg ved værkstedet.\",\n",
    "               \"Super service og en butik hvor man føler sig velkommen.\",\n",
    "               \"Anbefales!!\\nMin sælger var enormt forstående, det var første bil jeg skulle eje så der var lidt forskelligt men han tog vores bekymringer til sig. Hurtig svar og info vedrørende information om både levering og syn.\"\n",
    "    ]\n",
    "}\n",
    "review_list = [one_star_reviews, two_star_reviews, three_star_reviews, four_star_reviews, five_star_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "\n",
    "for dict in review_list:\n",
    "    for user, review in zip(dict['bruger'], dict['review']):\n",
    "\n",
    "        test_sequence_review = review\n",
    "\n",
    "        #convert prompt with template\n",
    "        test_prompt = prompt_template(user = user, review = test_sequence_review)\n",
    "\n",
    "        # Using pipeline\n",
    "        generator = pipeline(\"text-generation\", model= model, tokenizer=tokenizer)\n",
    "\n",
    "        # Generate text\n",
    "        test_reply_list = generator(test_prompt, max_length= 300, do_sample=True, temperature=0.4)\n",
    "\n",
    "        # Extract the generated reply\n",
    "        test_reply = (test_reply_list[0]['generated_text'])\n",
    "\n",
    "        # Strip the generated reply to exclude excess text\n",
    "        extracted_content = extract_first_lines(test_reply)\n",
    "\n",
    "        reply_object= []\n",
    "        reply = \"\"\n",
    "        # Print the extracted content\n",
    "        for tag, content in extracted_content.items():\n",
    "            reply += f\"{tag} {content}\"\n",
    "        reply_object.append(reply)\n",
    "        print(reply_object)\n",
    "        #replies.append(reply_object)\n",
    "\n",
    "        response = extracted_content['### Svar:']\n",
    "        responses.append('### Svar :' + response)\n",
    "\n",
    "        break\n",
    "\n",
    "print(responses)\n",
    "\n",
    "#df = pd.DataFrame(replies, columns=['User', 'Review', 'Reply'])\n",
    "#df.to_csv('generated_replies.csv', index=False, sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
